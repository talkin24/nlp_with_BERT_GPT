# 1. 처음 만나는 자연어 처리

## 1-1. 딥러닝 기반 자연어 처리 모델

## 1-2. 트랜스퍼 러닝

업스트림 태스크
- 다음 단어 맞히기: GPT 계열 업스트림 태스크, 해당 태스크로 학습된 모델을 "언어모델(language model)"이라고 함
- 빈칸 채우기: BERT 계열 업스트림 태스크, 해당 태스크로 학습된 모델을 "마스크 언어모델(masked language model)"이라고 함
  - 위 두가지 태스크는 자기지도학습(self-supervised learning)임

다운스트림 태스크
- 문서분류
- 자연어추론: 두 문장 사이의 관계 추론(참, 거짓, 중립 등)
- 개체명인식
- 질의응답
- 문장생성
  - 위 태스크들은 모두 파인튜닝 방식으로 학습됨
  - 그러나 실제로 다양한 다운스트림 태스크 학습 방식 존재
    - 파인튜닝(fine-tuning): 다운스트림 태스크 데이터 전체 사용. 데이터에 맞게 모델 전체 업데이트. 비용문제 있음
    - 프롬프트 튜닝(prompt tuning): 다운스트림 태스크 데이터 전체 사용. 데이터에 맞게 모델 일부만 업데이트
    - 인컨텍스트 러닝(in-context learning): 다운스트림 태스크 데이터 일부 사용. 모델을 업데이트 하지 않음
      - 제로샷 러닝(zero-shot learning): 데이터 0건 사용
      - 원샷 러닝(one-shot learning): 데이터 1건 사용
      - 퓨샷 러닝(few-shot learning): 데이터 몇건 사용


## 1-3. 학습 파이프라인 소개
1. 각종 설정값 정하기
   - args(lr, batch size, ...)
2. 데이터 내려받기
   - Korpora 패키지는 다양한 한국어 말뭉치 쉽게 내려받고 전처리하도록 도와줌
3. 프리트레인을 마친 모델 준비하기
4. 토크나이저 준비하기
5. 데이터 로더 준비하기
   - 동일한 배치 안에 있는 토큰만 길이가 같으면 됨
   - 컬레이트(collate): 배치의 모양 등을 정비해 모델의 최종 입력으롬나들어 주는 과정
     - list -> tensor 변환도  포함
6. 태스크 정의하기
   - 태스크 = (모델, 최적화방법, 학습과정 ...)
7. 모델 학습하기
   - trainer: 파이토치 라이트닝에서  제공하는 객체로 실제 학습을 수행
     - GPU 등 하드웨어 설정, 학습 기록 로깅, 체크포인트 저장 등 복잡한 설정을 알아서 해줌

## 1-4. 개발 환경 설정
- 구글 드라이브와 연결
  ```
  from google.colab import drive
  drive.mount('/gdrive', force_remount=True)
  ```


# 2. 문장을 작은 단위로 쪼개기

## 2-1. 토큰화란?
- 토큰화: 문장 -> 토큰 시퀀스
- 토큰화 개념을 넓은 의미로 해석할 때, 토큰 나누기 + 품사 부착
- 토큰화 방식
  1. 단어 단위 토큰화
     - 어휘 집합의 크기가 매우 커질 수 있음
  2. 문자 단위 토큰화
     - 어휘 집합의 크기 문제로부터 상대적으로 자유로움
     - 미등록 토큰 문제로부터 자유로움
     - BUT 각 문자 토큰은 의미 있는 단위가 되기 어려움
     - 또한 토큰 시퀀스의 길이가 상대적으로 김 -> 성능 악화
  3. 서브워드 단위 토큰화
     - 단어와 문자 단위 토큰화의 중간에 있는 형태 -> 둘의 장점만을 취함
     - 어휘 집합 크기가 지나치게 커지지 않으면서도 미등록 토큰 문제를 해결, 분석된 토큰 시퀀스가 너무 길어지지 않게 함
     - 대표적 기법: 바이트 페어 인코딩
  
## 2-2. 바이트 페어 인코딩(BPE)이란?
- GPT - BPE
- BERT - wordpiece
- BPE
  - 가장 많이 등장한 문자열을 병합하여 데이터를 압축하는 기법
  - 연속된 두 글자를 한 글자로 병합
  - 사전 크기 증가를 억제하면서도 정보를 효율적으로 압축할 수 있음
  - 분석 대상 언어에 대한 지식이 필요 없음
  - 절차
      1. 어휘 집합 구축: 고빈도 바이그램 쌍을 병합하는 방식으로 구축
        - 프리토크나이즈: 말뭉치 공백으로 분절
        - 문자 단위 초기 어휘집합 생성
        - 프리토크나이즈 결과를 초기 어휘 집합으로 재작성
        - 위 결과를 바이그램으로 묶어서 나열
        - 바이그램 쌍 중 가장 빈도 높은 것 어휘 집합에 추가
        - (반복) 해당 어휘집합으로 빈도표 재작성 -> 바이그램으로 묶어 빈도 가장 높은 것 어휘집합에 추가
          - 어휘 집합이 사용자가 정한 크기가 될 때까지 반복
      2. 토큰화
        - '어휘집합'과 '병합 우선순위'가 있으면 수행 가능
        - 프리토크나이즈 수행
        - 문자 단위 분리 -> 우선 순위 참고하여 병합
        - 병합 대상이 없을때까지 반복
        - 어휘집합에 없는 문자는 미등록 토큰(<unk>)
  

