# Hugginface - Tokenizers
- the Rust implemetation ë•ì— êµ‰ì¥íˆ ë¹ ë¥¸ í›ˆë ¨ê³¼ í† í¬ë‚˜ì´ì¦ˆê°€ ê°€ëŠ¥
- Full alignment tracking: íŒŒê´´ì ì¸ ì •ê·œí™”ë¥¼ í•˜ë”ë¼ë„ ì›ë˜ ë¬¸ì¥ì˜ ì¼ë¶€ë¶„ì„ í•­ìƒ ë˜ì°¾ì„ ìˆ˜ ìˆìŒ
- truancation, padding, add the special tokens ë“± ì „ì²˜ë¦¬ ê°€ëŠ¥

## Quick tour

BPEëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤í™”
```
from tokenizers import Tokenizer
from tokenizers.models import BPE

tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
```

í•™ìŠµì„ ìœ„í•´ì„œëŠ” trainer ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¨ì•¼í•¨
inputs = [vocab_size, min_frequency, special_tokens]
```
from tokenizers.trainers import BpeTrainer

trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
```
- special tokensì˜ ìˆœì„œ ë§¤ìš°ì¤‘ìš”
  - ìˆœì„œì— ë”°ë¼ IDê°€ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸
  - í˜„ì¬ ìˆœì„œëŠ” [UNK]: 0, [CLS]: 1

ì‚¬ì „ í† í°í™”ê¸°ë¥¼ ì‚¬ìš©í•˜ë©´ ì‚¬ì „ í† í°í™”ê¸°ì—ì„œ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë³´ë‹¤ í° í† í°ì€ ì•ˆë‚˜ì˜¤ê²Œ ë¨

```
# ê³µë°±ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê°€ì¥ ê°„ë‹¨í•œ pre-tokenizer
from tokenizers.pre_tokenizers import Whitespace

tokenizer.pre_tokenizer = Whitespace()
```

train ë©”ì†Œë“œì— fileê³¼ trainerë¥¼ ì…ë ¥

```
files = [f"data/wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
tokenizer.train(files, trainer)
```

tokenizerì˜ configurationê³¼ vocab ì €ì¥
```
tokenizer.save("data/tokenizer-wiki.json")
```

tokenizer ë¶ˆëŸ¬ì˜¤ê¸°
```
tokenizer = Tokenizer.from_file("data/tokenizer-wiki.json")
```

encode ë©”ì†Œë“œë¡œ í† í¬ë‚˜ì´ì € ì‚¬ìš© ê°€ëŠ¥
```
output = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")
```
=> 'Encoding' object ë¦¬í„´
tokens, ids ì†ì„±
```
print(output.tokens)
# ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?"]
print(output.ids)
# [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]
```
offsetìœ¼ë¡œ [UNK] í† í° ì›ë˜ ê°’ ì°¾ê¸° ê°€ëŠ¥ 
```
print(output.offsets[9])
# (26, 27)

sentence = "Hello, y'all! How are you ğŸ˜ ?"
sentence[26:27]
# "ğŸ˜"
```

Post-processing: [CLS], [SEP] í›„ ì‚½ì…
TemplateProcessingì´ ê°€ì¥ í”í•˜ê²Œ ì‚¬ìš©

ìš°ì„  ë”ë¸”ì²´í¬ë¥¼ ìœ„í•´ í† í° id ë¨¼ì € í™•ì¸
```
tokenizer.token_to_id("[SEP]")
# 2
```
traditional BERTë¥¼ ì£¼ëŠ” post-processing
```
from tokenizers.processors import TemplateProcessing

tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)
```
- '\$A'ê°€ sentenceë¥¼ ë‚˜íƒ€ëƒ„. $BëŠ” second sentence
- ë’¤ì—ì˜¤ëŠ” ':1'ì€ sentence ID. defaultëŠ” 0
- ìŠ¤í˜ì…œ í† í°ì„ vocabê³¼ ì—°ê²°

results
```
output = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")
print(output.tokens)
# ["[CLS]", "Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?", "[SEP]"]

output = tokenizer.encode("Hello, y'all!", "How are you ğŸ˜ ?")
print(output.tokens)
# ["[CLS]", "Hello", ",", "y", "'", "all", "!", "[SEP]", "How", "are", "you", "[UNK]", "?", "[SEP]"]

print(output.type_ids)
# [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]
```
- output.type_ids ëŠ” ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´

ìµœê³ ì˜ ì†ë„ë¥¼ ë‚´ê¸°ìœ„í•´ì„  'encode_batch()' ë©”ì†Œë“œë¡œ batchë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ
```
output = tokenizer.encode_batch(
    [["Hello, y'all!", "How are you ğŸ˜ ?"], ["Hello to you too!", "I'm fine, thank you!"]]
)
```
- ë¦¬ìŠ¤íŠ¸ë¡œ ì…ë ¥

enable_padding ë©”ì†Œë“œë¡œ íŒ¨ë“œí† í°ë§Œ ì…ë ¥í•˜ë©´, ë°°ì¹˜ ì¤‘ ê°€ì¥ ê¸´ ë¬¸ì¥ ê¸°ì¤€ìœ¼ë¡œ íŒ¨ë”© ì…ë ¥
```
tokenizer.enable_padding(pad_id=3, pad_token="[PAD]")
```
 - direction: íŒ¨ë”© ë°©í–¥ ì„¤ì •. ë””í´íŠ¸ëŠ” ì˜¤ë¥¸ìª½
 - length: ëª¨ë“  ë¬¸ì¥ ê¸¸ì´ ê³ ì •í•˜ê³  ì‹¶ì„ë•Œ ì§€ì •
  
```
output = tokenizer.encode_batch(["Hello, y'all!", "How are you ğŸ˜ ?"])
print(output[1].tokens)
# ["[CLS]", "How", "are", "you", "[UNK]", "?", "[SEP]", "[PAD]"]

print(output[1].attention_mask)
# [1, 1, 1, 1, 1, 1, 1, 0]
```
- attention_mask: ë¬¸ì¥ê³¼ íŒ¨ë”© êµ¬ë¶„

í”„ë¦¬íŠ¸ë ˆì¸ë“œ í† í¬ë‚˜ì´ì €
```
from tokenizers import Tokenizer

tokenizer = Tokenizer.from_pretrained("bert-base-uncased")
```

ì§ì ‘ vocab íŒŒì¼ì„ ë‹¤ìš´ë°›ì•„ì„œ ì‚¬ìš©ë„ ê°€ëŠ¥
```
!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt

from tokenizers import BertWordPieceTokenizer

tokenizer = BertWordPieceTokenizer("bert-base-uncased-vocab.txt", lowercase=True)
```