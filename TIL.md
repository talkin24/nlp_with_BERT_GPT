# 1. 처음 만나는 자연어 처리

## 1-1. 딥러닝 기반 자연어 처리 모델

## 1-2. 트랜스퍼 러닝

업스트림 태스크
- 다음 단어 맞히기: GPT 계열 업스트림 태스크, 해당 태스크로 학습된 모델을 "언어모델(language model)"이라고 함
- 빈칸 채우기: BERT 계열 업스트림 태스크, 해당 태스크로 학습된 모델을 "마스크 언어모델(masked language model)"이라고 함
  - 위 두가지 태스크는 자기지도학습(self-supervised learning)임

다운스트림 태스크
- 문서분류
- 자연어추론: 두 문장 사이의 관계 추론(참, 거짓, 중립 등)
- 개체명인식
- 질의응답
- 문장생성
  - 위 태스크들은 모두 파인튜닝 방식으로 학습됨
  - 그러나 실제로 다양한 다운스트림 태스크 학습 방식 존재
    - 파인튜닝(fine-tuning): 다운스트림 태스크 데이터 전체 사용. 데이터에 맞게 모델 전체 업데이트. 비용문제 있음
    - 프롬프트 튜닝(prompt tuning): 다운스트림 태스크 데이터 전체 사용. 데이터에 맞게 모델 일부만 업데이트
    - 인컨텍스트 러닝(in-context learning): 다운스트림 태스크 데이터 일부 사용. 모델을 업데이트 하지 않음
      - 제로샷 러닝(zero-shot learning): 데이터 0건 사용
      - 원샷 러닝(one-shot learning): 데이터 1건 사용
      - 퓨샷 러닝(few-shot learning): 데이터 몇건 사용


## 1-3. 학습 파이프라인 소개
1. 각종 설정값 정하기
   - args(lr, batch size, ...)
2. 데이터 내려받기
   - Korpora 패키지는 다양한 한국어 말뭉치 쉽게 내려받고 전처리하도록 도와줌
3. 프리트레인을 마친 모델 준비하기
4. 토크나이저 준비하기
5. 데이터 로더 준비하기
   - 동일한 배치 안에 있는 토큰만 길이가 같으면 됨
   - 컬레이트(collate): 배치의 모양 등을 정비해 모델의 최종 입력으롬나들어 주는 과정
     - list -> tensor 변환도  포함
6. 태스크 정의하기
   - 태스크 = (모델, 최적화방법, 학습과정 ...)
7. 모델 학습하기
   - trainer: 파이토치 라이트닝에서  제공하는 객체로 실제 학습을 수행
     - GPU 등 하드웨어 설정, 학습 기록 로깅, 체크포인트 저장 등 복잡한 설정을 알아서 해줌

## 1-4. 개발 환경 설정
- 구글 드라이브와 연결
  ```
  from google.colab import drive
  drive.mount('/gdrive', force_remount=True)
  ```


# 2. 문장을 작은 단위로 쪼개기

## 2-1. 토큰화란?
- 토큰화: 문장 -> 토큰 시퀀스
- 토큰화 개념을 넓은 의미로 해석할 때, 토큰 나누기 + 품사 부착
- 토큰화 방식
  1. 단어 단위 토큰화
     - 어휘 집합의 크기가 매우 커질 수 있음
  2. 문자 단위 토큰화
     - 어휘 집합의 크기 문제로부터 상대적으로 자유로움
     - 미등록 토큰 문제로부터 자유로움
     - BUT 각 문자 토큰은 의미 있는 단위가 되기 어려움
     - 또한 토큰 시퀀스의 길이가 상대적으로 김 -> 성능 악화
  3. 서브워드 단위 토큰화
     - 단어와 문자 단위 토큰화의 중간에 있는 형태 -> 둘의 장점만을 취함
     - 어휘 집합 크기가 지나치게 커지지 않으면서도 미등록 토큰 문제를 해결, 분석된 토큰 시퀀스가 너무 길어지지 않게 함
     - 대표적 기법: 바이트 페어 인코딩
  
## 2-2. 바이트 페어 인코딩(BPE)이란?
- GPT - BPE
- BERT - 워드피스
- BPE(빈도)
  - 가장 많이 등장한 문자열을 병합하여 데이터를 압축하는 기법
  - 연속된 두 글자를 한 글자로 병합
  - 사전 크기 증가를 억제하면서도 정보를 효율적으로 압축할 수 있음
  - 분석 대상 언어에 대한 지식이 필요 없음
  - 절차
      1. 어휘 집합 구축: 고빈도 바이그램 쌍을 병합하는 방식으로 구축
        - 프리토크나이즈: 말뭉치 공백으로 분절
        - 문자 단위 초기 어휘집합 생성
        - 프리토크나이즈 결과를 초기 어휘 집합으로 재작성
        - 위 결과를 바이그램으로 묶어서 나열
        - 바이그램 쌍 중 가장 빈도 높은 것 어휘 집합에 추가
        - (반복) 해당 어휘집합으로 빈도표 재작성 -> 바이그램으로 묶어 빈도 가장 높은 것 어휘집합에 추가
          - 어휘 집합이 사용자가 정한 크기가 될 때까지 반복
      2. 토큰화
        - '어휘집합'과 '병합 우선순위'가 있으면 수행 가능
        - 프리토크나이즈 수행
        - 문자 단위 분리 -> 우선 순위 참고하여 병합
        - 병합 대상이 없을때까지 반복
        - 어휘집합에 없는 문자는 미등록 토큰(<unk>)

- 워드피스(우도)
  - 말뭉치에서 자주 등장한 문자열을 토큰으로 인식한다는 점에서 BPE와 본질적으로 유사
  - 문자열 병합 기준에서 BPE와 차이를 보임
    - 단순 빈도 기준이 아닌, 병합 시 말뭉치의 likelihood를 가장 높이는 쌍을 병합
    - 워드피스에서는 병합 대상 전체 후보들 가운데 우도 계산값이 가장 높은 쌍을 합침
    - 따라서 워드피스는 '병합 우선순위' 필요 없이 '어휘집합'만 가지고 토큰화 가능
  - 분석 대상 어절에 '어휘집합'에 있는 서브워드가 포함된 경우 해당 서브워드를 어절에서 분리
    - 단, 후보가 여럿일 경우 가장 긴 서브워드 선택
    - 서브워드 후보가 하나도 없을 시에는 해당 문자열 전체를 미등록 단어로 취급
  

## 2-3. 어휘 집합 구축하기
- Korpora 패키지를 통해 말뭉치 다운로드 가능
- BPE: GPT 계열 모델이 사용하는 토크나이저 기법
  - 단 문자 단위가 아니라 '유니코드 바이트' 수준
    - 전세계 대부분의 글자는 유니코드로 표현할 수 있으므로 미등록 토큰 문제에서 비교적 자유로워짐
    - 한글은 한 글자가 3개의 유니코드 바이트로 표현됨
  - 말뭉치를 유니코드 바이트로 변환 -> 토큰화
  - 어휘집합 구축 -> return: vocab.json, merges
- wordpiece: BERT 계열이 사용하는 토크나이저
  - 어휘집합 구축 -> return: vocab.txt

## 2-4. 토큰화하기
- vocab, merges 파일로 사전학습된 토크나이저 사용 가능(BERT는 vocab만)
- GPT 토크나이저 
  - 인풋: 문장, 패딩 기준, 토큰 기준 최대 길이, 문장 잘림 허용 옵션(truncation)
  - 아웃풋: input_ids(토큰 인덱스), attention_mask(일반 토큰과 패딩 토큰 구분)
- BERT 토크나이저
  - 인풋: 문장, 패딩 기준, 토큰 기준 최대 길이, 문장 잘림 허용 옵션(truncation)
  - 아웃풋: input_ids(토큰 인덱스), attention_mask(일반 토큰과 패딩 토큰 구분), token_type_ids(세그먼트 정보, 첫번째문장 = 0, 두번째 문장 = 1, ...)
    - 세그먼트 정보 입력은 BERT 모델의 특징

