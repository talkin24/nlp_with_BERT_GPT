# 1. 처음 만나는 자연어 처리

## 1-1. 딥러닝 기반 자연어 처리 모델

## 1-2. 트랜스퍼 러닝

업스트림 태스크
- 다음 단어 맞히기: GPT 계열 업스트림 태스크, 해당 태스크로 학습된 모델을 "언어모델(language model)"이라고 함
- 빈칸 채우기: BERT 계열 업스트림 태스크, 해당 태스크로 학습된 모델을 "마스크 언어모델(masked language model)"이라고 함
  - 위 두가지 태스크는 자기지도학습(self-supervised learning)임

다운스트림 태스크
- 문서분류
- 자연어추론: 두 문장 사이의 관계 추론(참, 거짓, 중립 등)
- 개체명인식
- 질의응답
- 문장생성
  - 위 태스크들은 모두 파인튜닝 방식으로 학습됨
  - 그러나 실제로 다양한 다운스트림 태스크 학습 방식 존재
    - 파인튜닝(fine-tuning): 다운스트림 태스크 데이터 전체 사용. 데이터에 맞게 모델 전체 업데이트. 비용문제 있음
    - 프롬프트 튜닝(prompt tuning): 다운스트림 태스크 데이터 전체 사용. 데이터에 맞게 모델 일부만 업데이트
    - 인컨텍스트 러닝(in-context learning): 다운스트림 태스크 데이터 일부 사용. 모델을 업데이트 하지 않음
      - 제로샷 러닝(zero-shot learning): 데이터 0건 사용
      - 원샷 러닝(one-shot learning): 데이터 1건 사용
      - 퓨샷 러닝(few-shot learning): 데이터 몇건 사용


## 1-3. 학습 파이프라인 소개
1. 각종 설정값 정하기
   - args(lr, batch size, ...)
2. 데이터 내려받기
   - Korpora 패키지는 다양한 한국어 말뭉치 쉽게 내려받고 전처리하도록 도와줌
3. 프리트레인을 마친 모델 준비하기
4. 토크나이저 준비하기
5. 데이터 로더 준비하기
   - 동일한 배치 안에 있는 토큰만 길이가 같으면 됨
   - 컬레이트(collate): 배치의 모양 등을 정비해 모델의 최종 입력으롬나들어 주는 과정
     - list -> tensor 변환도  포함
6. 태스크 정의하기
   - 태스크 = (모델, 최적화방법, 학습과정 ...)
7. 모델 학습하기
   - trainer: 파이토치 라이트닝에서  제공하는 객체로 실제 학습을 수행
     - GPU 등 하드웨어 설정, 학습 기록 로깅, 체크포인트 저장 등 복잡한 설정을 알아서 해줌

## 1-4. 개발 환경 설정
- 구글 드라이브와 연결
  ```
  from google.colab import drive
  drive.mount('/gdrive', force_remount=True)
  ```


# 2. 문장을 작은 단위로 쪼개기

## 2-1. 토큰화란?
- 토큰화: 문장 -> 토큰 시퀀스
- 토큰화 개념을 넓은 의미로 해석할 때, 토큰 나누기 + 품사 부착
- 