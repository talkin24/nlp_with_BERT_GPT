# 1. 처음 만나는 자연어 처리

## 1-1. 딥러닝 기반 자연어 처리 모델

## 1-2. 트랜스퍼 러닝

업스트림 태스크
- 다음 단어 맞히기: GPT 계열 업스트림 태스크, 해당 태스크로 학습된 모델을 "언어모델(language model)"이라고 함
- 빈칸 채우기: BERT 계열 업스트림 태스크, 해당 태스크로 학습된 모델을 "마스크 언어모델(masked language model)"이라고 함
  - 위 두가지 태스크는 자기지도학습(self-supervised learning)임

다운스트림 태스크
- 문서분류
- 자연어추론: 두 문장 사이의 관계 추론(참, 거짓, 중립 등)
- 개체명인식
- 질의응답
- 문장생성
  - 위 태스크들은 모두 파인튜닝 방식으로 학습됨
  - 그러나 실제로 다양한 다운스트림 태스크 학습 방식 존재
    - 파인튜닝(fine-tuning): 다운스트림 태스크 데이터 전체 사용. 데이터에 맞게 모델 전체 업데이트. 비용문제 있음
    - 프롬프트 튜닝(prompt tuning): 다운스트림 태스크 데이터 전체 사용. 데이터에 맞게 모델 일부만 업데이트
    - 인컨텍스트 러닝(in-context learning): 다운스트림 태스크 데이터 일부 사용. 모델을 업데이트 하지 않음
      - 제로샷 러닝(zero-shot learning): 데이터 0건 사용
      - 원샷 러닝(one-shot learning): 데이터 1건 사용
      - 퓨샷 러닝(few-shot learning): 데이터 몇건 사용


## 1-3. 학습 파이프라인 소개
1. 각종 설정값 정하기
   - args(lr, batch size, ...)
2. 데이터 내려받기
   - Korpora 패키지는 다양한 한국어 말뭉치 쉽게 내려받고 전처리하도록 도와줌
3. 프리트레인을 마친 모델 준비하기
4. 토크나이저 준비하기
5. 데이터 로더 준비하기
   - 동일한 배치 안에 있는 토큰만 길이가 같으면 됨
   - 컬레이트(collate): 배치의 모양 등을 정비해 모델의 최종 입력으롬나들어 주는 과정
     - list -> tensor 변환도  포함
6. 태스크 정의하기
   - 태스크 = (모델, 최적화방법, 학습과정 ...)
7. 모델 학습하기
   - trainer: 파이토치 라이트닝에서  제공하는 객체로 실제 학습을 수행
     - GPU 등 하드웨어 설정, 학습 기록 로깅, 체크포인트 저장 등 복잡한 설정을 알아서 해줌

## 1-4. 개발 환경 설정
- 구글 드라이브와 연결
  ```
  from google.colab import drive
  drive.mount('/gdrive', force_remount=True)
  ```


# 2. 문장을 작은 단위로 쪼개기

## 2-1. 토큰화란?
- 토큰화: 문장 -> 토큰 시퀀스
- 토큰화 개념을 넓은 의미로 해석할 때, 토큰 나누기 + 품사 부착
- 토큰화 방식
  1. 단어 단위 토큰화
     - 어휘 집합의 크기가 매우 커질 수 있음
  2. 문자 단위 토큰화
     - 어휘 집합의 크기 문제로부터 상대적으로 자유로움
     - 미등록 토큰 문제로부터 자유로움
     - BUT 각 문자 토큰은 의미 있는 단위가 되기 어려움
     - 또한 토큰 시퀀스의 길이가 상대적으로 김 -> 성능 악화
  3. 서브워드 단위 토큰화
     - 단어와 문자 단위 토큰화의 중간에 있는 형태 -> 둘의 장점만을 취함
     - 어휘 집합 크기가 지나치게 커지지 않으면서도 미등록 토큰 문제를 해결, 분석된 토큰 시퀀스가 너무 길어지지 않게 함
     - 대표적 기법: 바이트 페어 인코딩
  
## 2-2. 바이트 페어 인코딩(BPE)이란?
- GPT - BPE
- BERT - 워드피스
- BPE(빈도)
  - 가장 많이 등장한 문자열을 병합하여 데이터를 압축하는 기법
  - 연속된 두 글자를 한 글자로 병합
  - 사전 크기 증가를 억제하면서도 정보를 효율적으로 압축할 수 있음
  - 분석 대상 언어에 대한 지식이 필요 없음
  - 절차
      1. 어휘 집합 구축: 고빈도 바이그램 쌍을 병합하는 방식으로 구축
        - 프리토크나이즈: 말뭉치 공백으로 분절
        - 문자 단위 초기 어휘집합 생성
        - 프리토크나이즈 결과를 초기 어휘 집합으로 재작성
        - 위 결과를 바이그램으로 묶어서 나열
        - 바이그램 쌍 중 가장 빈도 높은 것 어휘 집합에 추가
        - (반복) 해당 어휘집합으로 빈도표 재작성 -> 바이그램으로 묶어 빈도 가장 높은 것 어휘집합에 추가
          - 어휘 집합이 사용자가 정한 크기가 될 때까지 반복
      2. 토큰화
        - '어휘집합'과 '병합 우선순위'가 있으면 수행 가능
        - 프리토크나이즈 수행
        - 문자 단위 분리 -> 우선 순위 참고하여 병합
        - 병합 대상이 없을때까지 반복
        - 어휘집합에 없는 문자는 미등록 토큰(<unk>)

- 워드피스(우도)
  - 말뭉치에서 자주 등장한 문자열을 토큰으로 인식한다는 점에서 BPE와 본질적으로 유사
  - 문자열 병합 기준에서 BPE와 차이를 보임
    - 단순 빈도 기준이 아닌, 병합 시 말뭉치의 likelihood를 가장 높이는 쌍을 병합
    - 워드피스에서는 병합 대상 전체 후보들 가운데 우도 계산값이 가장 높은 쌍을 합침
    - 따라서 워드피스는 '병합 우선순위' 필요 없이 '어휘집합'만 가지고 토큰화 가능
  - 분석 대상 어절에 '어휘집합'에 있는 서브워드가 포함된 경우 해당 서브워드를 어절에서 분리
    - 단, 후보가 여럿일 경우 가장 긴 서브워드 선택
    - 서브워드 후보가 하나도 없을 시에는 해당 문자열 전체를 미등록 단어로 취급
  

## 2-3. 어휘 집합 구축하기
- Korpora 패키지를 통해 말뭉치 다운로드 가능
- BPE: GPT 계열 모델이 사용하는 토크나이저 기법
  - 단 문자 단위가 아니라 '유니코드 바이트' 수준
    - 전세계 대부분의 글자는 유니코드로 표현할 수 있으므로 미등록 토큰 문제에서 비교적 자유로워짐
    - 한글은 한 글자가 3개의 유니코드 바이트로 표현됨
  - 말뭉치를 유니코드 바이트로 변환 -> 토큰화
  - 어휘집합 구축 -> return: vocab.json, merges
- wordpiece: BERT 계열이 사용하는 토크나이저
  - 어휘집합 구축 -> return: vocab.txt

## 2-4. 토큰화하기
- vocab, merges 파일로 사전학습된 토크나이저 사용 가능(BERT는 vocab만)
- GPT 토크나이저 
  - 인풋: 문장, 패딩 기준, 토큰 기준 최대 길이, 문장 잘림 허용 옵션(truncation)
  - 아웃풋: input_ids(토큰 인덱스), attention_mask(일반 토큰과 패딩 토큰 구분)
- BERT 토크나이저
  - 인풋: 문장, 패딩 기준, 토큰 기준 최대 길이, 문장 잘림 허용 옵션(truncation)
  - 아웃풋: input_ids(토큰 인덱스), attention_mask(일반 토큰과 패딩 토큰 구분), token_type_ids(세그먼트 정보, 첫번째문장 = 0, 두번째 문장 = 1, ...)
    - 세그먼트 정보 입력은 BERT 모델의 특징

# 3. 숫자 세계로 떠난 자연어
## 3-1. 미리 학습된 언어모델
- 언어모델: 단어 시퀀스에 확률을 부여하는 모델 / 이전 단어들이 주어졌을 때 다음 단어가 나타날 확률을 부여하는 모델
- 순방향 언어모델(forward language model) - GPT, ELMo의 사전학습 방법
- 역방향 언어모델(backward language model) - ELMo의 사전학습 방법
- 넓은 의미의 언어모델: 컨텍스트가 전제된 상태에서 특정 단어가 나타날 조건부 확률
- 마스크 언어 모델엔 양방향 성질이 있음
- 스킵 그램 모델
  - 어떤 단어 앞뒤에 특정 범위를 정해둠
  - 이 범위 내에 어떤 단어들이 올지 분류
  - 컨텍스트로 설정한 단어 주변에 어떤 단어들이 분포해 있는지 학습
  - Word2Vec이 해당 모델 방식으로 학습
- 언어 모델이 주목받는 이유
  1. '다음단어맞히기', '빈칸맞히기'를 사용하면 레이블링 없이 학습데이터를 싼 값에 만들어 낼 수 있음
  2. 대량의 말뭉치로 언어모델의 프리트레인하면 다운스트림 태스크에서 적은 양의 데이터로 성능 큰 폭 향상 가능

## 3-2. 트랜스포머 살펴보기
- 트랜스포머는 sequence-to-sequence 모델
  - 특정 속성을 지닌 시퀀스를 다른 속성의 시퀀스로 변환하는 작업
  - 임의의 시퀀스를 해당 시퀀스와 속성이 다른 시퀀스로 변환하는 작업은 기계번역이 아니더라도 수행 가능
    - (ex. 기온의 시퀀스 -> 태풍 발생 여부의 시퀀스)
  - 인코더 - 디코더 2개 파트로 구성
- 인코더의 입력: 소스시퀀스 전체
- 디코더의 입력: 문맥(=인코더의 출력), 맞혀야할 단어 이전의 정답 타깃 시퀀스(훈련 시)/이전 디코더 출력(인퍼런스 시)
- 인코더와 디코더 차이: 마스크 멀티 헤드 어텐션, 인코더의 컨텍스트 반영
- 트랜스포머의 경쟁력은 셀프어텐션에 있다!
  - 어텐션: 중요한 요소에 집중하고 그렇지 않은 요소는 무시하기
  - 셀프어텐션: 입력 시퀀스 가운데 의미 있는 요소들 위주로 정보를 추출
    - CNN과 비교: CNN은 합성곱 필터 크기를 넘어서는 문맥은 읽어내기 어려움
    - RNN과 비교: 길이가 길어질수록 정보 압축 어려움. 오래전 입력 단어 잊거나 특정 정보 과도하게 반영(문장의 마지막 단어가 출력에 많이 반영됨)
    - 어텐션과 비교: 디코더 쪽 RNN에 어텐션을 추가. 디코더가 타깃 시퀀스를 생성할 때 소스 시퀀스 전체에서 어떤 요소에 주목해야 할 지를 알려줌
      - 어텐션은 소스 시퀀스 전체 단어들과 타깃 시퀀스 단어 하나 사이를 연결. 반면 셀프 어텐션은 입력 시퀀스 전체 단어들 사이를 연결
      - 셀프어텐션은 RNN 없이 동작
      - 어텐션은 타깃 언어의 단어 1개 생성 시 1회 수행, 셀프 어텐션은 인코더, 디코더 블록의 개수만큼 반복 수행
  
  

- 